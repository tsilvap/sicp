#+TITLE: Chapter 1: Building Abstractions With Procedures
#+DATE: 2023-08-28
#+MATH: true
#+LATEX_HEADER: \usepackage{amsmath}

* Sessions
** 1.2.6 Example: Testing for Primality
#+begin_src scheme :session 1.2.6 :results none
  (define (smallest-divisor n)
    (find-divisor n 2))

  (define (find-divisor n test-divisor)
    (cond ((> (square test-divisor) n) n)
          ((divides? test-divisor n) test-divisor)
          (else (find-divisor n (+ test-divisor 1)))))

  (define (divides? a b)
    (= (remainder b a) 0))

  (define (square n) (* n n))
#+end_src

#+begin_src scheme :session 1.2.6 :results none
  (define (prime? n)
    (= n (smallest-divisor n)))
#+end_src

#+begin_src scheme :session 1.2.6 :results none
  (define (expmod base exp m)
    (cond ((= exp 0) 1)
          ((even? exp)
           (remainder (square (expmod base (/ exp 2) m))
                      m))
          (else
           (remainder (* base (expmod base (- exp 1) m))
                      m))))
#+end_src

#+begin_src scheme :session 1.2.6 :results none
  (define (fermat-test n)
      (define (try-it a)
        (= (expmod a n n) a))
      (try-it (+ 1 (random (- n 1)))))
#+end_src

#+begin_src scheme :session 1.2.6 :results none
  (define (fast-prime? n times)
    (cond ((= times 0) #t)
          ((fermat-test n) (fast-prime? n (- times 1)))
          (else #f)))
#+end_src

* Notes
** 1.2.4 Exponentiation
*** Fast exponent calculation by successive squaring
An algorithm for computing \(b^n\) that grows logarithmically in space
and number of steps:

#+begin_src scheme :session
  (define (fast-expt b n)
    (cond ((= n 0) 1)
          ((even? n) (square (fast-expt b (/ n 2))))
          (else (* b (fast-expt b (- n 1))))))

  (define (even? n)
    (= (remainder n 2) 0))

  (define (square n) (* n n))
#+end_src

* Exercise 1.1
Below is a sequence of expressions. What is the result printed by the
interpreter in response to each expression? Assume that the sequence
is to be evaluated in the order in which it is presented.

#+begin_src scheme
  10
  (+ 5 3 4)
  (- 9 1)
  (/ 6 2)
  (+ (* 2 4) (- 4 6))
  (define a 3)
  (define b (+ a 1))
  (+ a b (* a b))
  (= a b)
  (if (and (> b a) (< b (* a b)))
      b
      a)
  (cond ((= a 4) 6)
        ((= b 4) (+ 6 7 a))
        (else 25))
  (+ 2 (if (> b a) b a))
  (* (cond ((> a b) a)
           ((< a b) b)
           (else -1))
     (+ a 1))
#+end_src

* Solution 1.1
The results are, in this order:

#+begin_example
  10
  12
  8
  3
  6
  #<unspecified>
  #<unspecified>
  19
  #f
  4
  16
  6
  16
#+end_example

* Exercise 1.2
Translate the following expression into prefix form:

\[
    \frac{5+4+(2-(3-(6+\frac{4}{5})))}{3(6-2)(2-7)}
\]

* Solution 1.2
#+begin_src scheme
  (/ (+ 5 4 (- 2 (- 3 (+ 6 (/ 4 5)))))
     (* 3 (- 6 2) (- 2 7)))
#+end_src

* Exercise 1.3
Define a procedure that takes three numbers as arguments and returns
the sum of the squares of the two larger numbers.

* Solution 1.3
#+begin_src scheme :session 1-3
  (define (sum-of-squares-of-two-largest x y z)
    (cond ((and (<= x y) (<= x z))
	   (sum-of-squares y z))
	  ((and (<= y x) (<= y z))
	   (sum-of-squares x z))
	  ((and (<= z x) (<= z y))
	   (sum-of-squares x y))))

  (define (sum-of-squares x y)
    (+ (* x x) (* y y)))
#+end_src

* Exercise 1.4
Observe that our model of evaluation allows for combinations whose
operators are compound expressions. Use this observation to describe
the behavior of the following procedure:

#+begin_src scheme
  (define (a-plus-abs-b a b)
    ((if (> b 0) + -) a b))
#+end_src

* TODO Solution 1.4

* Exercise 1.5
Ben Bitdiddle has invented a test to determine whether the interpreter
he is faced with is using applicative order evaluation or normal-order
evaluation. He defines the following two procedures:

#+begin_src scheme
  (define (p) (p))

  (define (test x y)
    (if (= x 0)
	0
	y))
#+end_src

Then he evaluates the expression

#+begin_src scheme
  (test 0 (p))
#+end_src

What behavior will Ben observe with an interpreter that uses
applicative-order evaluation? What behavior will he observe with an
interpreter that uses normal-order evaluation? Explain your answer.
(Assume that the evaluation rule for the special form ~if~ is the same
whether the interpreter is using normal or applicative order: The
predicate expression is evaluated first, and the result determines
whether to evaluate the consequent or the alternative expression.)

* TODO Solution 1.5

* Exercise 1.6
Alyssa P. Hacker doesn't see why ~if~ needs to be provided as a special
form. "Why can't I just define it as an ordinary procedure in terms of
~cond~?" she asks. Alyssa's friend Eva Lu Ator claims this can indeed be
done, and she defines a new version of ~if~:

#+begin_src scheme :session 1-6
  (define (new-if predicate
		  then-clause
		  else-clause)
    (cond (predicate then-clause)
	  (else else-clause)))
#+end_src

Eva demonstrates the program for Alyssa:

#+begin_src scheme :session 1-6
  (new-if (= 2 3) 0 5)
#+end_src

#+begin_src scheme :session 1-6
  (new-if (= 1 1) 0 5)
#+end_src

Delighted, Alyssa uses ~new-if~ to rewrite the square-root program:

#+begin_src scheme :session 1-6
  (define (sqrt-iter guess x)
    (new-if (good-enough? guess x)
	    guess
	    (sqrt-iter (improve guess x) x)))
#+end_src

What happens when Alyssa attempts to use this to compute square roots?
Explain.

* TODO Solution 1.6

* Exercise 1.7
The ~good-enough?~ test used in computing square roots will not be very
effective for finding the square roots of very small numbers. Also, in
real computers, arithmetic operations are almost always performed with
limited precision. This makes our test inadequate for very large
numbers. Explain these statements, with examples showing how the test
fails for small and large numbers. An alternative strategy for
implementing ~good-enough?~ is to watch how ~guess~ changes from one
iteration to the next and to stop when the change is a very small
fraction of the guess. Design a square-root procedure that uses this
kind of end test. Does this work better for small and large numbers?

* TODO Solution 1.7

* Exercise 1.8
Newton's method for cube roots is based on the fact that if \(y\) is
an approximation to the cube root of \(x\), then a better
approximation is given by the value

\[
    \frac{x/y^2 + 2y}{3}
\]

Use this formula to implement a cube-root procedure analogous to the
square-root procedure. (In 1.3.4 we will see how to implement
Newton's method in general as an abstraction of the square-root and
cube-root procedures.)

* Solution 1.8
#+begin_src scheme :session 1-8
  (define (cbrt-iter guess x)
    (if (good-enough? guess x)
	guess
	(cbrt-iter (improve guess x) x)))

  (define (improve guess x)
    (/ (+ (/ x (* guess guess)) (* 2 guess)) 3))

  (define (good-enough? guess x)
    (< (abs (- (cube guess) x)) 0.001))

  (define (cube x)
    (* x x x))

  (define (cbrt x)
    (cbrt-iter 1.0 x))
#+end_src

* Exercise 1.9
Each of the following two procedures defines a method for adding two
positive integers in terms of the procedures ~inc~, which increments its
argument by 1, and ~dec~, which decrements its argument by 1.

#+begin_src scheme
  (define (+ a b)
    (if (= a 0)
	b
	(inc (+ (dec a) b))))
#+end_src

#+begin_src scheme
  (define (+ a b)
    (if (= a 0)
	b
	(+ (dec a) (inc b))))
#+end_src

Using the substitution model, illustrate the process generated by each
procedure in evaluating ~(+ 4 5)~. Are these processes iterative or
recursive?

* TODO Solution 1.9

* Exercise 1.10
The following procedure computes a mathematical function called
Ackermann's function.

#+begin_src scheme
  (define (A x y)
    (cond ((= y 0) 0)
          ((= x 0) (* 2 y))
          ((= y 1) 2)
          (else (A (- x 1)
                   (A x (- y 1))))))
#+end_src

What are the values of the following expressions?

#+begin_src scheme
  (A 1 10)
  (A 2 4)
  (A 3 3)
#+end_src

Consider the following procedures, where ~A~ is the procedure defined
above:

#+begin_src scheme
  (define (f n) (A 0 n))
  (define (g n) (A 1 n))
  (define (h n) (A 2 n))
  (define (k n) (* 5 n n))
#+end_src

Give concise mathematical definitions for the functions computed by
the procedures ~f~, ~g~, and ~h~ for positive integer values of \(n\). For
example, ~(k n)~ computes \(5n^2\).

* TODO Solution 1.10

* Exercise 1.11
A function \(f\) is defined by the rule that \(f(n) = n\) if \(n < 3\) and
\(f(n) = f(n - 1) + 2f(n - 2) + 3f(n - 3)\) if \(n \geq 3\). Write a
procedure that computes \(f\) by means of a recursive process. Write a
procedure that computes \(f\) by means of an iterative process.

* Solution 1.11
Recursive process:

#+begin_src scheme
  (define (f n)
    (if (< n 3)
	n
	(+ (f (- n 1))
	   (* 2 (f (- n 2)))
	   (* 3 (f (- n 3))))))
#+end_src

** TODO Iterative process

* Exercise 1.12
The following pattern of numbers is called Pascal's triangle.

#+begin_example
          1
        1   1
      1   2   1
    1   3   3   1
  1   4   6   4   1
#+end_example

The numbers at the edge of the triangle are all 1, and each number inside the
triangle is the sum of the two numbers above it. Write a procedure that computes
elements of Pascal's triangle by means of a recursive process.

* Solution 1.12
#+begin_src scheme :session
  (define (pascal row column)
    (cond ((= column 1) 1)
          ((= column row) 1)
          (else (+ (pascal (1- row) (1- column))
                   (pascal (1- row) column)))))
#+end_src

* Exercise 1.13
Prove that \(\mathrm{Fib}(n)\) is the closest integer to \(\phi^{n} /
\sqrt{5}\), where \(\phi = (1 + \sqrt{5}) / 2\). Hint: Let \(\psi =
(1 - \sqrt{5}) / 2\). Use induction and the definition of the
Fibonacci numbers (see 1.2.2) to prove that \(\mathrm{Fib}(n) =
(\phi^n - \psi^n) / \sqrt{5}\).

* Solution 1.13
*Proposition 1.* If \( \mathrm{Fib}(k) = (\phi^k - \psi^k) / \sqrt{5} \)
and \( \mathrm{Fib}(k+1) = (\phi^{k+1} - \psi^{k+1}) / \sqrt{5} \),
for some integer \(k\), then
\( \mathrm{Fib}(k+2) = (\phi^{k+2} - \psi^{k+2}) / \sqrt{5} \).

*Proof.*

\[
    \begin{align*}
     \mathrm{Fib}(k+2)
     &= \mathrm{Fib}(k+1) + \mathrm{Fib}(k) \\
     &= \frac{\phi^{k+1} - \psi^{k+1}}{\sqrt{5}} + \frac{\phi^k - \psi^k}{\sqrt{5}} \\
     &= \frac{\phi^k(\phi + 1) - \psi^k(\psi + 1)}{\sqrt{5}} \\
     &= \frac{\phi^{k+2} - \psi^{k+2}}{\sqrt{5}}
    \end{align*}
\]

since \(\phi^2 = \phi + 1\) and \(\psi^2 = \psi + 1\).

*Proposition 2.* \( \mathrm{Fib}(n) = (\phi^n - \psi^n) / \sqrt{5} \),
for all natural \(n\).

*Proof.* It holds for \(n = 0\):

\[
    (\phi^0 - \psi^0)/\sqrt{5} = (1 - 1)/\sqrt{5} = 0
\]

which is \(\mathrm{Fib}(0)\).

And also for \(n = 1\):

\[
    \frac{\phi^1 - \psi^1}{\sqrt{5}}
    = \frac{(1 + \sqrt{5})/2 - (1-\sqrt{5})/2}{\sqrt{5}}
    = \frac{(2 \sqrt{5}) / 2}{\sqrt{5}}
    = 1
\]

which is \(\mathrm{Fib}(1)\).

Then, as a consequence of *Proposition 1*, it should hold for all
successors of \(1\) as well.

*Proposition 3.* \( \lvert \phi^n / \sqrt{5} - \mathrm{Fib}(n) \rvert < 1/2 \).

*Proof:*

\[
    \begin{align*}
     \lvert \phi^n / \sqrt{5} - \mathrm{Fib}(n) \rvert
     &= \lvert \frac{ \phi^n }{ \sqrt{5} } - \frac { \phi^n - \psi^n }{ \sqrt{5} } \rvert \\
     &= \frac{ \lvert \psi^n \lvert }{ \sqrt{5} } \\
     &= \frac{ \lvert 1 - \sqrt{5} \rvert^n  }{ 2^n \sqrt{5} } \\
     &< \frac{ 2^n }{ 2^n \sqrt{5} } = \frac{1}{ \sqrt{5} } < \frac{1}{2}
    \end{align*}
\]

We already know \(\mathrm{Fib}(n)\) is an integer from the definition
of the Fibonacci sequence, and so with *Proposition 3* we've proved
\(\mathrm{Fib}(n)\) is the closest integer to \(\phi^{n} / \sqrt{5}\).


* Exercise 1.14
Draw the tree illustrating the process generated by the ~count-change~
procedure of 1.2.2 in making change for 11 cents. What are the orders
of growth of the space and number of steps used by this process as the
amount to be changed increases?

* Solution 1.14
** Recursion tree
[[../images/1-14-tree.svg]]

** Space complexity
The space complexity is \(\Theta(n)\) just like the Fibonacci
tree-recursive algorithm: we only need to keep track of the nodes
above us.

** TODO Number of steps complexity
Got this insight from: https://zthomae.github.io/sicp/c1e14.html

insight: The ~kinds-of-coins~ argument is very annoying, if we start
from 5. But does this argument also obey some sort of pattern?  Start
analyzing with ~kinds-of-coins~ equal to 1 (rather easy), and analyze
what happens with ~kinds-of-coins~ equal to 2, and 3, and so on.

(See G. Polya: "*If you cannot solve the proposed problem* do not let
this failure afflict you too much but try to find consolation with
some easier success, /try to solve first some related problem/; [...]")

* Exercise 1.15
The sine of an angle (specified in radians) can be computed by making
use of the approximation \(\sin{x} \approx x\) if \(x\) is
sufficiently small, and the trigonometric identity

\[
    \sin{x} = 3\sin{\frac{x}{3}} - 4\sin^3{\frac{x}{3}}
\]

to reduce the size of the argument of \(\sin\). (For purposes of this
exercise an angle is considered “sufficiently small” if its magnitude
is not greater than 0.1 radians.)  These ideas are incorporated in the
following procedures:

#+begin_src scheme
  (define (cube x) (* x x x))

  (define (p x) (- (* 3 x) (* 4 (cube x))))

  (define (sine angle)
    (if (not (> (abs angle) 0.1))
        angle
        (p (sine (/ angle 3.0)))))
#+end_src

1. How many times is the procedure ~p~ applied when ~(sine 12.15)~ is
   evaluated?

1. What is the order of growth in space and number of steps (as a
   function of /a/) used by the process generated by the sine procedure
   when ~(sine a)~ is evaluated?

* Solution 1.15

1. Five times. While ~angle~ is not smaller than 0.1, we divide ~angle~ by
   three and call ~p~ on the sine of that value. We'll have to divide
   12.15 by three five times before it is smaller than 0.1 and ~sine~ no
   longer calls ~p~:

   #+begin_example
     (sine 12.15)
     (p (sine 4.05))
     (p (p (sine 1.35)))
     (p (p (p (sine 0.45))))
     (p (p (p (p (sine 0.15)))))
     (p (p (p (p (p (sine 0.05))))))
     (p (p (p (p (p 0.05)))))
   #+end_example

1. We have to divide /a/ by three approximately \(\log_{3}{10a}\) times
   before it is smaller than \(0.1\). Ignoring constant factors, the
   order of growth in both space and number of steps is then
   \(\Theta(\log{a})\).

* Exercise 1.16
Design a procedure that evolves an iterative exponentiation process
that uses successive squaring and uses a logarithmic number of steps,
as does ~fast-expt~. (Hint: Using the observation that \( (b^{n/2})^2
=(b^2)^{n/2} \), keep, along with the exponent \(n\) and the base
\(b\), an additional state variable \(a\), and define the state
transformation in such a way that the product \(ab^n\) is unchanged
from state to state. At the beginning of the process \(a\) is taken to
be \(1\), and the answer is given by the value of \(a\) at the end of
the process. In general, the technique of defining an /invariant
quantity/ that remains unchanged from state to state is a powerful way
to think about the design of iterative algorithms.)

* Solution 1.16
#+begin_src scheme
  (define (fast-expt b n)
    (fast-expt-iter 1 b n))

  (define (fast-expt-iter a b n)
    (cond ((= n 0) a)
          ((even? n) (fast-expt-iter a (square b) (/ n 2)))
          (else (fast-expt-iter (* a b) b (- n 1)))))
#+end_src

* Exercise 1.17
The exponentiation algorithms in this section are based on performing
exponentiation by means of repeated multiplication. In a similar way,
one can perform integer multiplication by means of repeated addition.
The following multiplication procedure (in which it is assumed that
our language can only add, not multiply) is analogous to the ~expt~
procedure:

#+begin_src scheme
  (define (* a b)
    (if (= b 0)
        0
        (+ a (* a (- b 1)))))
#+end_src

This algorithm takes a number of steps that is linear in ~b~. Now
suppose we include, together with addition, operations ~double~, which
doubles an integer, and ~halve~, which divides an (even) integer by 2.
Using these, design a multiplication procedure analogous to fast-expt
that uses a logarithmic number of steps.

* Solution 1.17
#+begin_src scheme
  (define (fast-mult a b)
    (cond ((= b 0) 0)
          ((even? b) (fast-mult (double a) (halve b)))
          (else (+ a (fast-mult a (- b 1))))))
#+end_src

* Exercise 1.18
Using the results of exercises 1.16 and 1.17, devise a procedure that
generates an iterative process for multiplying two integers in terms
of adding, doubling, and halving and uses a logarithmic number of
steps.

* Solution 1.18
#+begin_src scheme
  (define (fast-mult a b)
    (fast-mult-iter a b 0))

  (define (fast-mult-iter a b sum)
    (cond ((= b 0) sum)
          ((even? b) (fast-mult-iter (double a) (halve b) sum))
          (else (fast-mult-iter a (- b 1) (+ sum a)))))
#+end_src

* Exercise 1.19
There is a clever algorithm for computing the Fibonacci numbers in a
logarithmic number of steps. Recall the transformation of the state
variables \(a\) and \(b\) in the ~fib-iter~ process of section 1.2.2:
\(a \leftarrow a + b\) and \(b \leftarrow a\). Call this
transformation \(T\), and observe that applying \(T\) over and over
again \(n\) times, starting with \(1\) and \(0\), produces the pair
\(\mathrm{Fib}(n+1)\) and \(\mathrm{Fib}(n)\). In other words, the
Fibonacci numbers are produced by applying \(T^n\), the nth power of
the transformation \(T\), starting with the pair \((1, 0)\). Now
consider \(T\) to be the special case of \(p = 0\) and \(q = 1\) in a
family of transformations \(T_{pq}\), where \(T_{pq}\) transforms the
pair \((a, b)\) according to \(a \leftarrow bq + aq + ap\) and \(b
\leftarrow bp + aq\). Show that if we apply such a transformation
\(T_{pq}\) twice, the effect is the same as using a single
transformation \(T_{p^{\prime}q^{\prime}}\) of the same form, and
compute \(p^{\prime}\) and \(q^{\prime}\) in terms of \(p\) and \(q\).
This gives us an explicit way to square these transformations, and
thus we can compute \(T^n\) using successive squaring, as in the
~fast-expt~ procedure. Put this all together to complete the following
procedure, which runs in a logarithmic number of steps:

#+begin_src scheme
  (define (fib n)
    (fib-iter 1 0 0 1 n))

  (define (fib-iter a b p q count)
    (cond ((= count 0) b)
          ((even? count)
           (fib-iter a
                     b
                     (??)       ; compute p'
                     (??)       ; compute q'
                     (/ count 2)))
          (else (fib-iter (+ (* b q) (* a q) (* a p))
                          (+ (* b p) (* a q))
                          p
                          q
                          (- count 1)))))
#+end_src

* Solution 1.19
\[
    \begin{align*}
     T^2_{pq}(a,b)
     &= ((bp+aq)q + (bq+aq+ap)q + (bq+aq+ap)p, (bp+aq)p + (bq+aq+ap)q) \\
     &= (bpq+aq^2 + bq^2+aq^2+apq + bpq+apq+ap^2, bp^2+apq + bq^2+aq^2+apq) \\
     &= (b(q^2+2pq) + a(q^2+2pq) + a(p^2+q^2), b(p^2+q^2) + a(q^2+2pq))
    \end{align*}
\]

So, \(p^{\prime} = p^2+q^2\) and \(q^{\prime} = q^2+2pq\), and the
algorithm becomes:

#+begin_src scheme
  (define (fib n)
    (fib-iter 1 0 0 1 n))

  (define (fib-iter a b p q count)
    (cond ((= count 0) b)
          ((even? count)
           (fib-iter a
                     b
                     (+ (* p p) (* q q))
                     (+ (* q q) (* 2 p q))
                     (/ count 2)))
          (else (fib-iter (+ (* b q) (* a q) (* a p))
                          (+ (* b p) (* a q))
                          p
                          q
                          (- count 1)))))
#+end_src

* Exercise 1.20
The process that a procedure generates is of course dependent on the
rules used by the interpreter. As an example, consider the iterative
~gcd~ procedure given above. Suppose we were to interpret this procedure
using normal-order evaluation, as discussed in section 1.1.5. (The
normal-order-evaluation rule for ~if~ is described in exercise 1.5.)
Using the substitution method (for normal order), illustrate the
process generated in evaluating ~(gcd 206 40)~ and indicate the
~remainder~ operations that are actually performed. How many ~remainder~
operations are actually performed in the normal-order evaluation of
~(gcd 206 40)~? In the applicative-order evaluation?

* Solution 1.20
Normal-order evaluation:
#+begin_src scheme
  (gcd 206 40)
  ;; (if (= 40 0) ...), the else-branch will be evaluated.
  (gcd 40 (remainder 206 40))
  ;; (if (= (remainder 206 40) 0) ...), remainder operation is performed
  ;; 1x, and the else-branch will be evaluated.
  (gcd (remainder 206 40) (remainder 40 (remainder 206 40)))
  ;; (if (= (remainder 40 (remainder 206 40)) 0) ...), remainder
  ;; operation is performed 2x, and the else-branch will be evaluated.
  (gcd (remainder 40 (remainder 206 40))
       (remainder (remainder 206 40) (remainder 40 (remainder 206 40))))
  ;; (if (= (remainder (remainder 206 40) (remainder 40 (remainder 206
  ;; 40))) 0) ...), remainder operation is performed 4x, and the
  ;; else-branch will be evaluated.
  (gcd (remainder (remainder 206 40) (remainder 40 (remainder 206 40)))
       (remainder (remainder 40 (remainder 206 40))
                  (remainder (remainder 206 40) (remainder 40 (remainder 206 40)))))
  ;; Lastly, remainder operation is performed 7x, and the then-branch of
  ;; the if will be evaluated.
  (remainder (remainder 206 40) (remainder 40 (remainder 206 40)))
  ;; remainder operation is performed 4x before the final result:
  2
#+end_src

From the comments, we can see that ~remainder~ was performed:
1 + 2 + 4 + 7 + 4 = 18 times.

Applicative-order evaluation:
#+begin_src scheme
  (gcd 206 40)
  ;; (if (= 40 0) ...), the else-branch will be evaluated.
  (gcd 40 (remainder 206 40))
  ;; remainder operation is performed 1x as the arguments are evaluated.
  (gcd 40 6)
  ;; (if (= 6 0) ...), the else-branch will be evaluated.
  (gcd 6 (remainder 40 6))
  ;; remainder operation is performed 1x as the arguments are evaluated.
  (gcd 6 4)
  ;; (if (= 4 0) ...), the else-branch will be evaluated.
  (gcd 4 (remainder 6 4))
  ;; remainder operation is performed 1x as the arguments are evaluated.
  (gcd 4 2)
  ;; (if (= 2 0) ...), the else-branch will be evaluated.
  (gcd 2 (remainder 4 2))
  ;; remainder operation is performed 1x as the arguments are evaluated.
  (gcd 2 0)
  ;; (if (= 0 0) ...), the then-branch will be evaluated.
  2
#+end_src

In this version, ~remainder~ is performed: 1 + 1 + 1 + 1 = 4 times.

* Exercise 1.21
Use the ~smallest-divisor~ procedure to find the smallest divisor of
each of the following numbers: 199, 1999, 19999.

* Solution 1.21
#+begin_src scheme :session 1.2.7
  (list (smallest-divisor 199)
        (smallest-divisor 1999)
        (smallest-divisor 19999))
#+end_src

#+RESULTS:

* Exercise 1.22
Most Lisp implementations include a primitive called ~runtime~ that
returns an integer that specifies the amount of time the system has
been running (measured, for example, in microseconds). The following
~timed-prime-test~ procedure, when called with an integer \(n\), prints
\(n\) and checks to see if \(n\) is prime. If \(n\) is prime, the
procedure prints three asterisks followed by the amount of time used
in performing the test.

#+begin_src scheme :session 1.2.6 :results none
  (define (timed-prime-test n)
    (newline)
    (display n)
    (start-prime-test n (runtime)))

  (define (start-prime-test n start-time)
    (if (prime? n)
        (report-prime (- (runtime) start-time))))

  (define (report-prime elapsed-time)
    (display " *** ")
    (display elapsed-time))
#+end_src

Using this procedure, write a procedure ~search-for-primes~ that checks
the primality of consecutive odd integers in a specified range. Use
your procedure to find the three smallest primes larger than \(1000\);
larger than \(10\,000\); larger than \(100\,000\); larger than
\(1\,000\,000\). Note the time needed to test each prime. Since the
testing algorithm has order of growth of \(\Theta(\sqrt{n})\), you
should expect that testing for primes around \(10\,000\) should take
about \(\sqrt{10}\) times as long as testing for primes around
\(1000\). Do your timing data bear this out? How well do the data for
\(100\,000\) and \(1\,000\,000\) support the \(\Theta(\sqrt{n})\)
prediction? Is your result compatible with the notion that programs on
your machine run in time proportional to the number of steps required
for the computation?

* Solution 1.22
GNU Guile doesn't have a ~runtime~ primitive, but it has ~gettimeofday~
which we can use to compute the elapsed time:

#+begin_src scheme :session 1.2.6 :results none
  (define (current-time-usec)
    (let* ((tm (gettimeofday))
           (sec (car tm))
           (usec (cdr tm)))
      (+ (* (expt 10 6) sec) usec)))

  (define (timed-prime-test n)
    (newline)
    (display n)
    (start-prime-test n (current-time-usec)))

  (define (start-prime-test n start-time)
    (if (prime? n)
        (report-prime (- (current-time-usec)
                         start-time))))

#+end_src

Now, we can define ~search-for-primes~:

#+begin_src scheme :session 1.2.6 :results none
  (define (search-for-primes start end)
    (cond ((>= start end) 0)
          ((even? start) (search-for-primes (+ start 1) end))
          (else (timed-prime-test start)
                (search-for-primes (+ start 2) end))))
#+end_src

Running ~prime?~ on the suggested inputs is near-instant, so instead
we start from \(10^9\). Here's the three smallest primes in each
range, and the respective elapsed time:

#+begin_example
  1000000007 *** 606
  1000000009 *** 616
  1000000021 *** 599

  10000000019 *** 1979
  10000000033 *** 2074
  10000000061 *** 1943

  100000000003 *** 6147
  100000000019 *** 6242
  100000000057 *** 6205
#+end_example

Each time we increase the inputs by \(10\) times, the elapsed times
increase by about \(3.2\) times, which is very close to \(\sqrt{10}
\approx 3.1623\), so the data supports the \(\Theta{\sqrt{n}}\)
prediction very well, and the result is compatible with the notion
that programs on a machine run in time proportional to the number of
steps required for the computation.

* Exercise 1.23
The ~smallest-divisor~ procedure shown at the start of this section does
lots of needless testing: After it checks to see if the number is
divisible by 2 there is no point in checking to see if it is divisible
by any larger even numbers. This suggests that the values used for
~test-divisor~ should not be 2, 3, 4, 5, 6, …, but rather 2, 3, 5, 7, 9,
…. To implement this change, define a procedure ~next~ that returns 3 if
its input is equal to 2 and otherwise returns its input plus 2. Modify
the ~smallest-divisor~ procedure to use ~(next test-divisor)~ instead of
~(+ test-divisor 1)~. With ~timed-prime-test~ incorporating this modified
version of ~smallest-divisor~, run the test for each of the 12 primes
found in exercise 1.22. Since this modification halves the number of
test steps, you should expect it to run about twice as fast. Is this
expectation confirmed? If not, what is the observed ratio of the
speeds of the two algorithms, and how do you explain the fact that it
is different from 2?

* Solution 1.23
#+begin_src scheme :session 1.2.6 :results none
  (define (next n)
    (if (= n 2) 3 (+ n 2)))

  (define (find-divisor n test-divisor)
    (cond ((> (square test-divisor) n) n)
          ((divides? test-divisor n) test-divisor)
          (else (find-divisor n (next test-divisor)))))
#+end_src

And now we can run ~timed-prime-test~ on the 12 primes we had:
#+begin_src scheme :session 1.2.6 :results output
  (timed-prime-test 1000000007)
  (timed-prime-test 1000000009)
  (timed-prime-test 1000000021)

  (timed-prime-test 10000000019)
  (timed-prime-test 10000000033)
  (timed-prime-test 10000000061)

  (timed-prime-test 100000000003)
  (timed-prime-test 100000000019)
  (timed-prime-test 100000000057)
#+end_src

#+RESULTS:
#+begin_example

1000000007 *** 372
1000000009 *** 387
1000000021 *** 399
10000000019 *** 1244
10000000033 *** 1259
10000000061 *** 1232
100000000003 *** 3904
100000000019 *** 3857
100000000057 *** 3824
#+end_example

The expectation is not confirmed. The modified algorithm only runs
approximately 1.59 times faster than the original one.
** TODO Why? (Hint: Probably the extra function calls in ~next~)

* Exercise 1.24
Modify the ~timed-prime-test~ procedure of exercise 1.22 to use
~fast-prime?~ (the Fermat method), and test each of the 12 primes you
found in that exercise. Since the Fermat test has \(\Theta(\log{n})\)
growth, how would you expect the time to test primes near
\(1\,000\,000\) to compare with the time needed to test primes near
\(1000\)? Do your data bear this out? Can you explain any discrepancy
you find?

* Solution 1.24
We apply the Fermat's test with 10 tries for each of the numbers:

#+begin_src scheme :session 1.2.6 :results output
  (define (start-prime-test n start-time)
    (if (fast-prime? n 10)
        (report-prime (- (current-time-usec)
                         start-time))))

  (timed-prime-test 1000000007)
  (timed-prime-test 1000000009)
  (timed-prime-test 1000000021)
  (timed-prime-test 10000000019)
  (timed-prime-test 10000000033)
  (timed-prime-test 10000000061)
  (timed-prime-test 100000000003)
  (timed-prime-test 100000000019)
  (timed-prime-test 100000000057)
#+end_src

#+RESULTS:
#+begin_example

1000000007 *** 28
1000000009 *** 21
1000000021 *** 21
10000000019 *** 90
10000000033 *** 78
10000000061 *** 88
100000000003 *** 84
100000000019 *** 86
100000000057 *** 89
#+end_example

10*n -> log(10*n) = log(10) + log(n)
100*n -> log(10^2*n) = 2*log(10) + log(n)

* Exercise 1.25
Alyssa P. Hacker complains that we went to a lot of extra work in
writing ~expmod~. After all, she says, since we already know how to
compute exponentials, we could have simply written

#+begin_src scheme
  (define (expmod base exp m)
    (remainder (fast-expt base exp) m))
#+end_src

Is she correct? Would this procedure serve as well for our fast prime
tester? Explain.

* TODO Solution 1.25

* Exercise 1.26
Louis Reasoner is having great difficulty doing exercise 1.24. His
~fast-prime?~ test seems to run more slowly than his ~prime?~ test. Louis
calls his friend Eva Lu Ator over to help. When they examine Louis’s
code, they find that he has rewritten the ~expmod~ procedure to use an
explicit multiplication, rather than calling square:

#+begin_src scheme
  (define (expmod base exp m)
    (cond ((= exp 0) 1)
          ((even? exp)
           (remainder (* (expmod base (/ exp 2) m)
                         (expmod base (/ exp 2) m))
                      m))
          (else
           (remainder (* base (expmod base (- exp 1) m))
                      m))))
#+end_src

“I don’t see what difference that could make,” says Louis. “I do.”
says Eva. “By writing the procedure like that, you have transformed
the \(\Theta(\log{n})\) process into a \(\Theta(n)\) process.” Explain.

* TODO Solution 1.26

* Exercise 1.27
Demonstrate that the Carmichael numbers listed in footnote 47 really
do fool the Fermat test. That is, write a procedure that takes an
integer \(n\) and tests whether \(a^n\) is congruent to \(a\) modulo
\(n\) for every \(a < n\), and try your procedure on the given
Carmichael numbers.

* TODO Solution 1.27

* Exercise 1.28
One variant of the Fermat test that cannot be fooled is called the
/Miller-Rabin test/ (Miller 1976; Rabin 1980). This starts from an
alternate form of Fermat’s Little Theorem, which states that if \(n\)
is a prime number and \(a\) is any positive integer less than \(n\),
then a raised to the \((n − 1)\)-st power is congruent to \(1\) modulo
\(n\). To test the primality of a number \(n\) by the Miller-Rabin
test, we pick a random number \(a < n\) and raise \(a\) to the
\((n−1)\)-st power modulo \(n\) using the ~expmod~ procedure. However,
whenever we perform the squaring step in ~expmod~, we check to see if we
have discovered a “nontrivial square root of \(1\) modulo \(n\),” that
is, a number not equal to \(1\) or \(n − 1\) whose square is equal to
\(1\) modulo \(n\). It is possible to prove that if such a nontrivial
square root of \(1\) exists, then \(n\) is not prime. It is also
possible to prove that if \(n\) is an odd number that is not prime,
then, for at least half the numbers \(a < n\), computing \(a^{n − 1}\)
in this way will reveal a nontrivial square root of \(1\) modulo
\(n\). (This is why the Miller-Rabin test cannot be fooled.) Modify
the ~expmod~ procedure to signal if it discovers a nontrivial square
root of \(1\), and use this to implement the Miller-Rabin test with a
procedure analogous to ~fermat-test~. Check your procedure by testing
various known primes and non-primes. Hint: One convenient way to make
~expmod~ signal is to have it return \(0\).

* TODO Solution 1.28
